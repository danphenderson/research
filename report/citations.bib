@book{book1,
  author    = {Köppl, Tobias and Helmig, Rainer},
  title     = {Dimension Reduced Modeling of Blood Flow in Large Arteries},
  subtitle  = {An Introduction for Master Students and First Year Doctoral Students},
  year      = {2023},
  publisher = {Springer Nature Switzerland},
}

@book{book2,
  author    = {Formaggia, Luca and Quarteroni, Alfio},
  title     = {Mathematical Modeling and Numerical Simulation of the Cardiovascular System},
  subtitle  = {},
  year      = {2002},
  publisher = {},
}

@article{ehab332,
  author  = {Derimay, Francois and Finet, Gerard and Rioufol, Gilles},
  title   = {Coronary Artery Stenosis Prediction Does Not Mean Coronary Artery Stenosis Obstruction},
  journal = {European Heart Journal},
  year    = {2021},
  doi     = {10.1093/eurheartj/ehab332},
  url     = {https://watermark.silverchair.com/ehab332.pdf},
}


@article{bviscosity,
  author = {Pries AR, Neuhaus D, Gaehtgens P},
  journal = {Am J Physiol.},
  number = {263},
  title = {Blood viscosity in tube flow: dependence on diameter and hematocrit},
  volume = {(6 Pt 2)},
  year = {1992},
  doi = {10.1152/ajpheart.1992.263.6.H1770}
}

@article{ns-formulation,
  author = {John, L. and Pustějovská, P. and Steinbach, O.},
  title = {On the influence of the wall shear stress vector form on hemodynamic indicators},
  journal = {Computing and Visualization in Science},
  volume = {4},
  year = {2017},
  pages = {113-122},
  doi = {10.1007/s00791-017-0277-7}
}

@book{hemodynamics,
  author = {Galdi, Giovanni P. and Robertson, Anne M. and Rannacher, Rolf and Turek, Stefan},
  editor = {},
  publisher = {Birkhäuser Basel},
  title = {Hemodynamical Flows: Modeling, Analysis and Simulation},
  year = {2008},
  doi = {https://doi.org/10.1007/978-3-7643-7806-6}
}

@article{RevelsLubinPapamarkou2016,
    title = {Forward-Mode Automatic Differentiation in {J}ulia},
   author = {{Revels}, J. and {Lubin}, M. and {Papamarkou}, T.},
  journal = {arXiv:1607.07892 [cs.MS]},
     year = {2016},
      url = {https://arxiv.org/abs/1607.07892}
}

@article{Lagaris1998,
copyright = {1998 INIST-CNRS},
issn = {1045-9227},
journal = {IEEE transactions on neural networks},
keywords = {Partial differential equations ;  Applied sciences ;  Artificial intelligence ;  Artificial neural networks ;  Boundary conditions ;  Boundary value problems ;  Computer science;  control theory;  systems ;  Connectionism. Neural networks ;  Differential equations ;  Digital signal processors ;  Exact sciences and technology ;  Feedforward neural networks ;  Finite element methods ;  Moment methods ;  Neural networks ;  Problem solving game playing},
language = {eng},
number = {5},
pages = {987-1000},
publisher = {IEEE},
title = {Artificial neural networks for solving ordinary and partial differential equations},
volume = {9},
year = {1998},
abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.},
author = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
address = {New York, NY},
}

@article{RaissiM2019,
copyright = {2018 Elsevier Inc.},
issn = {0021-9991},
journal = {Journal of computational physics},
keywords = {Nonlinear dynamics ;  Partial differential equations ;  Artificial intelligence ;  Computational fluid dynamics ;  Computational physics ;  Data-driven scientific computing ;  Deep learning ;  Inverse problems ;  Machine learning ;  Mathematical models ;  Neural networks ;  Nonlinear differential equations ;  Nonlinear equations ;  Predictive modeling ;  Quantum mechanics ;  Runge-Kutta method ;  Runge–Kutta methods ;  Water waves ;  Wave propagation},
language = {eng},
pages = {686-707},
publisher = {Elsevier Inc},
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
volume = {378},
year = {2019},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.
•We put forth a deep learning framework that enables the synergistic combination of mathematical models and data.•We introduce an effective mechanism for regularizing the training of deep neural networks in small data regimes.•The proposed methods enable scientific prediction and discovery from incomplete models and incomplete data.},
author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
address = {Cambridge},
}
